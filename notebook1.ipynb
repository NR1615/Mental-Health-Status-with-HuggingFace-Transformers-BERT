{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOVyb1LfOJLd"
      },
      "outputs": [],
      "source": [
        "pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import dataloader\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vdemKwY-OWsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "ekDrsOULRbN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# List the contents of the directory to find the actual CSV file\n",
        "dataset_path = \"/kaggle/input/sentiment-analysis-for-mental-health\"\n",
        "# print(os.listdir(dataset_path)) # No longer needed after identifying the file\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(os.path.join(dataset_path, 'Combined Data.csv'))\n",
        "data"
      ],
      "metadata": {
        "id": "LdSw0SylRjfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(inplace=True)\n",
        "data.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "data"
      ],
      "metadata": {
        "id": "4Lx8plhdSVSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.sample(n=6000, random_state=42).reset_index(drop=True)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "cvsC5nh-Sxx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "AhnlessnTFeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords') # Download stopwords corpus\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "def clean_statement(text):\n",
        "  text=text.lower()\n",
        "\n",
        "  text=re.sub(r\"[^a-zA-Z\\s]\",'',text)\n",
        "\n",
        "  words=text.split()\n",
        "  words=[word for word in words if word not in stop_words]\n",
        "\n",
        "  return \" \".join(words)\n",
        "\n",
        "data['statement']=data['statement'].apply(clean_statement)\n",
        "data"
      ],
      "metadata": {
        "id": "EXhe7m4fTIyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['status'].value_counts()"
      ],
      "metadata": {
        "id": "3Y4Bek3AVxay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to balance dataset there are three ways\n",
        "1) oversampling\n",
        "2) undersampling"
      ],
      "metadata": {
        "id": "CmHaiQulV5WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros=RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "x=data.drop('status', axis=1)\n",
        "y=data['status']\n",
        "\n",
        "x_resampled, y_resampled=ros.fit_resample(x,y)\n",
        "\n",
        "data=pd.concat([x_resampled, y_resampled], axis=1)\n",
        "\n",
        "print(data['status'].value_counts())"
      ],
      "metadata": {
        "id": "EFDreuMdWHVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding"
      ],
      "metadata": {
        "id": "bwDjKkUTXTpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder=LabelEncoder()\n",
        "data['label']=label_encoder.fit_transform(data['status'])\n",
        "data\n"
      ],
      "metadata": {
        "id": "hrCnXBV8XVWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels=train_test_split(data['statement'], data['label'], test_size=0.2)"
      ],
      "metadata": {
        "id": "QtfSo4QYj-Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization ( llm ) ( pre-trained tokenizer BERT)"
      ],
      "metadata": {
        "id": "i9pVHbHIXkxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_texts), padding=True, truncation=True, max_length=200)\n",
        "test_encodings = tokenizer(list(test_texts), padding=True, truncation=True, max_length=200)"
      ],
      "metadata": {
        "id": "5irF2jeoj0Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels.to_list()})\n",
        "test_dataset=Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'labels': test_labels.to_list()})"
      ],
      "metadata": {
        "id": "1jkotIyM4ANJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tuning the Model"
      ],
      "metadata": {
        "id": "4yT5kurs5LTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\", # Corrected from evaluation_strategy to eval_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,                       # Enables Mixed Precision (Massive speedup on T4)\n",
        "    per_device_train_batch_size=32,  # Doubled from 16\n",
        "    per_device_eval_batch_size=32,   # Doubled from 16\n",
        "    gradient_accumulation_steps=1,   # Ensure this is the only instance of this argument\n",
        "    dataloader_num_workers=2,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=3\n",
        ")\n",
        "\n",
        "trainer=Trainer(model=model, args= training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=test_dataset)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "O_ibDaeE5Qvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, _, _ = trainer.predict(test_dataset)\n",
        "predicted_labels=np.argmax(predictions, axis=1)\n",
        "print(classification_report(test_labels, predicted_labels,target_names=label_encoder.classes_))\n",
        "cm=confusion_matrix(test_labels, predicted_labels)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZZlqy8OsUp97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/saved_mental_bert\")"
      ],
      "metadata": {
        "id": "QicrPS4Cm0yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/saved_mental_status_bert\")"
      ],
      "metadata": {
        "id": "oK3S02HNn41p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(label_encoder, open('/content/drive/MyDrive/label_encoder.pkl', 'wb'))\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model=AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/saved_mental_bert\")\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/saved_mental_status_bert\")\n",
        "#\n",
        "label_encoder=pickle.load(open('/content/drive/MyDrive/label_encoder.pkl', 'rb'))\n",
        "#"
      ],
      "metadata": {
        "id": "cG-58WHbn_dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now testing the model\n"
      ],
      "metadata": {
        "id": "7wEKf32Kq6yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def dectection(text):\n",
        "  text=clean_statement(text)\n",
        "  inputs=tokenizer(text, padding=True, truncation=True, max_length=200, return_tensors='pt')\n",
        "  outputs=model(**inputs)\n",
        "  logits=outputs.logits\n",
        "  predicted_labels=torch.argmax(logits, dim=1).item()\n",
        "\n",
        "  return label_encoder.inverse_transform([predicted_labels])[0]\n",
        "\n",
        "text=\"i am not okay i dont feel nice\"\n",
        "dectection(text)"
      ],
      "metadata": {
        "id": "-K3kDi0dpbdp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}